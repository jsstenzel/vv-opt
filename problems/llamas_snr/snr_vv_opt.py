import sys
import os
import scipy.stats
import math
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import csv
import dill

sys.path.append('../..')
#focal plane
from problems.verification.problem import *
#analysis
from obed.obed_multivar import *
from obed.obed_gbi import *
from obed.pdf_estimation import *
from uq.uncertainty_propagation import *
#from uq.sensitivity_analysis import *
from opt.ngsa import *

################################
#Useful definitions
################################

def proposal_fn_gamma(theta_curr, proposal_width):
	theta_prop = [0] * len(theta_curr)
	for i,_ in enumerate(theta_prop):
		#proposal dists are gammas, to match
		mean = abs(theta_curr[i])
		stddev = proposal_width
		variance = [w**2 for w in stddev]
		alpha = mean**2 / variance
		beta = mean / variance
		theta_prop[i] = scipy.stats.gamma.rvs(size=1, a=alpha, scale=1.0/beta)[0]
	return theta_prop
	
def proposal_fn_norm(theta_curr, proposal_width):
	theta_prop = [0] * len(theta_curr)
	for i,_ in enumerate(theta_prop):
		#proposal dists are gammas, to match
		mean = abs(theta_curr[i])
		stddev = proposal_width[i]
		theta_prop[i] = scipy.stats.norm.rvs(size=1, loc=mean, scale=stddev)[0]
	return theta_prop

req = 4.38 #max noise

theta_nominal = []
QoI_nominal = problem.H(theta_nominal)

d_historical = [
			   ]
		
y_nominal = likelihood_fn(dict(zip(problem.theta_names, theta_nominal)), dict(zip(problem.d_names, d_historical)), dict(zip(problem.x_names, problem.x_default)), err=False)
#print(y_nominal)

################################
#Analysis functions
################################

def vv_nominal(problem):
	print("QoI requirement:", req)
	print("Nominal QoI:", QoI_nominal)


###uncertainty analysis
def vv_UP_QoI(problem, req):
	#uncertainty propagation of HLVA
	uq_thetas = problem.prior_rvs(10000)
	Qs = [problem.H(theta) for theta in uq_thetas]
	uncertainty_prop_plot([theta[0] for theta in uq_thetas], xlab="Gain [ADU/e-]")
	uncertainty_prop_plot([theta[1] for theta in uq_thetas], xlab="Read noise [e-]")
	uncertainty_prop_plot([theta[2] for theta in uq_thetas], xlab="Dark current [e-/s]")
	uncertainty_prop_plot(Qs, xlab="QoI: Avg. Noise [e-]", vline=[req])

	#prob of meeting req along priors:
	count_meetreq = 0
	for Q in Qs:
		if Q <= req:
			count_meetreq += 1
	prob_meetreq = count_meetreq / len(Qs)
	print("Probability of meeting requirement given priors:", prob_meetreq)

"""
#sensitivity analysis of HLVA
def vv_SA_QoI():
	#it'll be straightforward to see the dependence of QoI on theta
	Si = sobol_saltelli(problem.H, 
						2**5, #SALib wants powers of 2 for convergence
						var_names=problem.theta_names, 
						var_dists=[prior[0] for prior in problem.priors], 
						var_bounds=[prior[1] for prior in problem.priors], 
						conf = 0.95, doSijCalc=False, doPlot=True, doPrint=True)
"""

#Uncertainty analysis of the experiment models
def vv_UP_exp(problem, dd, savefig=False):
	print("Likelihood distribution for nominal historical case:",flush=True)
	tt = problem.prior_rvs(1); print(tt)
	ysample_nominal = [problem.eta(tt, dd) for _ in range(10000)]
	uncertainty_prop_plots(ysample_nominal, xlabs=["Y0","Y1","Y2"], saveFig='UP_exp_nominal' if savefig else '')
	likelihood,_ = general_likelihood_kernel(ysample_nominal)
	#kde_plot(likelihood, ysample_nominal, plotStyle='together') #needs fixing?
	
	#Also plot the y's generated by the joint distribution p(y|theta,d)p(theta)
	print("Experiments simulated from the joint distribution:",flush=True)
	uq_thetas = problem.prior_rvs(10000)
	uq_ys = [problem.eta(theta, dd) for theta in uq_thetas]
	uncertainty_prop_plot([y[0] for y in uq_ys], xlab="Y0 (joint distribution)", c='orchid', saveFig='UP_joint_y0' if savefig else '')
	uncertainty_prop_plot([y[1] for y in uq_ys], xlab="Y1 (joint distribution)", c='orchid', saveFig='UP_joint_y1.png' if savefig else '')
	uncertainty_prop_plot([y[2] for y in uq_ys], xlab="Y2 (joint distribution)", c='orchid', saveFig='UP_joint_y2' if savefig else '')

#"""	These cause problems on eofe8
#sensitivity analysis of the experiment models
def vv_SA_exp(problem, dd):
		#we want to see sensitivity of each yi to their inputs, theta and x
		#instead of trying to break eta into its constitutent experiments, i think i want to just use all of eta,
		#and doing separate sensitivity analysis on each of the y[i] coming from theta, over entire x and theta span
		#sobol_saltelli expects a function that takes a single list of parameters
		def eta_1(param): #gain
			gain = param[0]
			rn = param[1]
			dc = param[2]
			_x = dict(zip(problem.x_names, problem.x_default)) 
			_x["sigma_dc"] = param[3]
			_x["P_signal"] = param[4]
			_x["P_noise"] = param[5]
			_x["T_ccd"] = param[6]
			_x["sigma_E"] = param[7]
			_x["w"] = param[8]
			_x["activity_cd109"] = param[9]
			y = gain_exp(gain, rn, dc, dd[0], dd[1], _x, err=True)
			return y
		def eta_2(param): #rn
			gain = param[0]
			rn = param[1]
			_x = dict(zip(problem.x_names, problem.x_default)) 
			_x["sigma_stray"] = param[2]
			_x["sigma_dc"] = param[3]
			y = read_noise_exp(gain, rn, dd[2], _x, err=True)
			return y
		def eta_3(param): #dc
			gain = param[0]
			rn = param[1]
			dc = param[2]
			_x = dict(zip(problem.x_names, problem.x_default)) 
			_x["sigma_stray"] = param[3]
			_x["sigma_dc"] = param[4]
			y = dark_current_exp(gain, rn, dc, dd[3], dd[4], dd[5], _x, err=True)
			return y
		#use partials to define these later?

		expvar_names = problem.theta_names + problem.x_names
		expvar_dists = [prior[0] for prior in problem.priors] + [prior[0] for prior in problem.x_dists]
		expvar_bounds=[prior[1] for prior in problem.priors] + [prior[1] for prior in problem.x_dists]
			
		#so ugly!!!
		Si_1 = sobol_saltelli(eta_1, 
							2**8, #SALib wants powers of 2 for convergence
							var_names=[x for i,x in enumerate(expvar_names) if i in [0,1,2,5,8,9,10,12,13,14]], 
							var_dists=[x for i,x in enumerate(expvar_dists) if i in [0,1,2,5,8,9,10,12,13,14]], 
							var_bounds=[x for i,x in enumerate(expvar_bounds) if i in [0,1,2,5,8,9,10,12,13,14]], 
							conf = 0.99, doSijCalc=False, doPlot=True, doPrint=True)
		
		Si_2 = sobol_saltelli(eta_2, 
							2**8, #SALib wants powers of 2 for convergence
							var_names=[x for i,x in enumerate(expvar_names) if i in [0,1,5,7]], 
							var_dists=[x for i,x in enumerate(expvar_dists) if i in [0,1,5,7]], 
							var_bounds=[x for i,x in enumerate(expvar_bounds) if i in [0,1,5,7]], 
							conf = 0.99, doSijCalc=False, doPlot=True, doPrint=True)
		
		Si_3 = sobol_saltelli(eta_3, 
							2**8, #SALib wants powers of 2 for convergence
							var_names=[x for i,x in enumerate(expvar_names) if i in [0,1,2,5,7]], 
							var_dists=[x for i,x in enumerate(expvar_dists) if i in [0,1,2,5,7]], 
							var_bounds=[x for i,x in enumerate(expvar_bounds) if i in [0,1,2,5,7]], 
							conf = 0.99, doSijCalc=False, doPlot=True, doPrint=True)	
#"""						

def vv_gbi_test(problem, d, Yd, N):		
	theta_train = problem.prior_rvs(N)
	qoi_train = [problem.H(theta) for theta in theta_train]
	y_train = [problem.eta(theta, d) for theta in theta_train]
	
	gmm = gbi_train_model(theta_train, qoi_train, y_train, verbose=2, ncomp=8)
	
	a,b,c = gbi_condition_model(gmm, Yd, verbose=2)
	
	plot_predictive_posterior(a, b, c, 0, 7, drawplot=True)
	
def vv_gbi_rand_test(problem, d, N):		
	theta_train = problem.prior_rvs(N)
	qoi_train = [problem.H(theta) for theta in theta_train]
	y_train = [problem.eta(theta, d) for theta in theta_train]
	
	gmm = gbi_train_model(theta_train, qoi_train, y_train, verbose=0, ncomp=8)
	
	truth_theta = problem.prior_rvs(1)
	measured_y = problem.eta(truth_theta, d)
	a,b,c = gbi_condition_model(gmm, measured_y, verbose=0)
	
	plot_predictive_posterior(a, b, c, 0, 7, drawplot=False, plotmean=True)
	plt.axvline(problem.H(truth_theta), c='blue')
	plt.show()

def vv_obed_gbi(problem, d):
	U, U_list = U_varH_gbi(d, fp, n_mc=10**5, n_gmm=10**5, doPrint=True)
	print(U)
	#print(U_list)
	uncertainty_prop_plot(U_list, c='royalblue', xlab="specific U")#, saveFig='OBEDresult')
	return U
	
def uncertainty_mc(problem):
	util_samples = []
	for ii in range(1000):
		print(ii, flush=True)
		util = U_varH_gbi(d_historical, fp, n_mc=5*10**3, n_gmm=10**3, ncomp=5, doPrint=False)
		util_samples.append(util)
		
	uncertainty_prop_plot(util_samples, c='purple', xlab="utility for d=d_hist")
	print(statistics.variance(util_samples))

if __name__ == '__main__':  
	###Problem Definition
	alphas = [1,1,1,10,10,10,100,100,100]
	betas = [.1,1,10,.1,1,10,.1,1,10]
	problem = simple_mass_problem_def(alphas, betas)
	
	d_best = [0.001]*len(alphas)
	d_worst = [10]*len(alphas)

	###Uncertainty Quantification
	vv_nominal(problem)
	
	vv_UP_QoI(problem, 350)
	
	vv_SA_QoI(problem)
	
	vv_UP_exp(problem, d_best)
	
	vv_SA_exp(problem, d_best)
	

	###Optimal Bayesian Experimental Design
	vv_gbi_test(problem, d_best, y_nominal, 10**6)
	
	vv_gbi_rand_test(problem, d_best, 10**4)
	
	U_hist = vv_obed_gbi(problem, d_best)
	
	uncertainty_mc(problem)
		
		
	costs, utilities, designs = ngsa2_problem_parallel(8, problem, hours=0, minutes=0, popSize=12, nMonteCarlo=5*10**3, nGMM=5*10**3)
	plot_ngsa2(costs, utilities, showPlot=True, savePlot=False, logPlotXY=[False,False])
	
