import sys
import os
import scipy.stats
import math
import matplotlib.pyplot as plt
import numpy as np
import csv

sys.path.append('../..')
#focal plane
from problems.fp_verification.fp_problem import *
#analysis
from obed.obed_gbi import *
from uq.uncertainty_propagation import *
from uq.plotmatrix import *
from approx.joint_model_reduction import *
#from uq.sensitivity_analysis import *
from opt.ngsa import *

################################
#Useful definitions
################################

req = 4.38 #max noise

theta_nominal = [1.1, 2.5, .001]
QoI_nominal = fp.H(theta_nominal)

d_historical = [
				20,   #t_gain
				30,   #I_gain
				1,	#n_meas_rn
				8,	#d_num
				9600, #d_max
				2	 #d_pow   #approx
			   ]
			   
d_best = [
				600,   #t_gain
				100,   #I_gain
				50,	#n_meas_rn
				100,	#d_num
				12000, #d_max
				3	 #d_pow   #approx
		]
			   
d_worst = [
				1,   #t_gain
				1,   #I_gain
				1,	#n_meas_rn
				2,	#d_num
				1, #d_max
				0.1	 #d_pow   #approx
		]
		
y_nominal = fp_likelihood_fn(dict(zip(fp.theta_names, theta_nominal)), dict(zip(fp.d_names, d_historical)), dict(zip(fp.x_names, fp.x_default)), err=False)
#print(y_nominal)

################################
#Analysis functions
################################

def fp_vv_nominal():
	print("QoI requirement:", req)
	print("Nominal QoI:", QoI_nominal)


###uncertainty analysis
def fp_vv_UP_QoI(req):
	#uncertainty propagation of HLVA
	uq_thetas = fp.prior_rvs(10000)
	Qs = [fp.H(theta) for theta in uq_thetas]
	uncertainty_prop_plot([theta[0] for theta in uq_thetas], xlab="Gain [ADU/e-]")
	uncertainty_prop_plot([theta[1] for theta in uq_thetas], xlab="Read noise [e-]")
	uncertainty_prop_plot([theta[2] for theta in uq_thetas], xlab="Dark current [e-/s]")
	uncertainty_prop_plot(Qs, xlab="QoI: Avg. Noise [e-]", vline=[req])

	#prob of meeting req along priors:
	count_meetreq = 0
	for Q in Qs:
		if Q <= req:
			count_meetreq += 1
	prob_meetreq = count_meetreq / len(Qs)
	print("Probability of meeting requirement given priors:", prob_meetreq)

"""
#sensitivity analysis of HLVA
def fp_vv_SA_QoI():
	#it'll be straightforward to see the dependence of QoI on theta
	Si = sobol_saltelli(fp.H, 
						2**5, #SALib wants powers of 2 for convergence
						var_names=fp.theta_names, 
						var_dists=[prior[0] for prior in fp.priors], 
						var_bounds=[prior[1] for prior in fp.priors], 
						conf = 0.95, doSijCalc=False, doPlot=True, doPrint=True)
"""

#Uncertainty analysis of the experiment models
def fp_vv_UP_exp(dd, savefig=False):
	print("Likelihood distribution for nominal historical case:",flush=True)
	tt = fp.prior_rvs(1); print(tt)
	ysample_nominal = [fp.eta(tt, dd) for _ in range(10000)]
	uncertainty_prop_plots(ysample_nominal, xlabs=["Y0","Y1","Y2"], saveFig='UP_exp_nominal' if savefig else '')
	likelihood,_ = general_likelihood_kernel(ysample_nominal)
	#kde_plot(likelihood, ysample_nominal, plotStyle='together') #needs fixing?
	
	#Also plot the y's generated by the joint distribution p(y|theta,d)p(theta)
	print("Experiments simulated from the joint distribution:",flush=True)
	uq_thetas = fp.prior_rvs(10000)
	uq_ys = [fp.eta(theta, dd) for theta in uq_thetas]
	uncertainty_prop_plot([y[0] for y in uq_ys], xlab="Y0 (joint distribution)", c='orchid', saveFig='UP_joint_y0' if savefig else '')
	uncertainty_prop_plot([y[1] for y in uq_ys], xlab="Y1 (joint distribution)", c='orchid', saveFig='UP_joint_y1.png' if savefig else '')
	uncertainty_prop_plot([y[2] for y in uq_ys], xlab="Y2 (joint distribution)", c='orchid', saveFig='UP_joint_y2' if savefig else '')


###obed analysis -- naive multiprocessing approach
def fp_vv_obed_1machine():
	print("Generating kernel",flush=True)
	n_kde_axis = 47 #47^3 equals about 10^5
	kde_gains = np.linspace(0,3,n_kde_axis)
	kde_rn = np.linspace(1,4,n_kde_axis)
	kde_dc = np.linspace(0,.01,n_kde_axis)
	kde_thetas = np.vstack((np.meshgrid(kde_gains, kde_rn, kde_dc))).reshape(3,-1).T #thanks https://stackoverflow.com/questions/18253210/creating-a-numpy-array-of-3d-coordinates-from-three-1d-arrays
	kde_ys = [fp.eta(theta, d_historical) for theta in kde_thetas]
	likelihood_kernel, kde_ythetas = general_likelihood_kernel(kde_thetas, kde_ys)

	print("starting obed",flush=True)
	prop_width = [0.007167594573520732, 0.17849464019335232, 0.0006344271319903282] #stddev from last study

	U, U_list = U_probreq_multi(d_historical, fp, proposal_fn_norm, prop_width, likelihood_kernel, req=3.0, n_mc=1000, n_mcmc=2000, burnin=300, lag=1, doPrint=True)
	print(U)
	print(U_list)
	uncertainty_prop_plot(U_list, c='royalblue', xlab="specific U", saveFig='OBEDresult')
	
###obed analysis -- robust for sbatch, one step of the monte carlo
def fp_vv_obed_cluster(d, kernel_pkl):
	print("starting obed",flush=True)
	prop_width = [0.007167594573520732, 0.17849464019335232, 0.0006344271319903282] #stddev from last study

	U = U_probreq_1step(d_historical, fp, proposal_fn_norm, prop_width, kernel_pkl, req=3.0, n_mcmc=2000, burnin=300, lag=1, doPrint=True)

def fp_vv_plot_obed_results(file):
	u = []
	H_mean = []
	H_stddev = []
	with open(file) as csvfile:
		csvreader = csv.reader(csvfile, delimiter=',')
		for row in csvreader:
			u.append(float(row[0]))
			H_mean.append(float(row[4]))
			H_stddev.append(float(row[5]))
	
	uncertainty_prop_plot(u, c='royalblue', xlab="specific U from MC")
	uncertainty_prop_plot(H_mean, c='royalblue', xlab="H means from MC")
	uncertainty_prop_plot(H_stddev, c='royalblue', xlab="H stddevs from MC")
	print("MC-mean probability of meeting requirement:",np.mean(u))
	print("MC-mean of H_posterior mean:",np.mean(H_mean))
	print("   Probability of MC-mean meeting requirement:",len([1 for hmean in H_mean if hmean<3.0])/len(H_mean))
	print("MC-mean of H_posterior stddev:",np.mean(H_stddev))
	
	plt.scatter(H_mean, H_stddev)
	plt.show()
	
def fp_vv_gbi_test(d, Yd, N):		
	theta_train = fp.prior_rvs(N)
	qoi_train = [fp.H(theta) for theta in theta_train]
	y_train = [fp.eta(theta, d) for theta in theta_train]
	
	gmm = gbi_train_model(theta_train, qoi_train, y_train, verbose=2, ncomp=8)
	
	a,b,c = gbi_condition_model(gmm, Yd, verbose=2)
	
	plot_predictive_posterior(a, b, c, 0, 7, drawplot=True)
	
def fp_vv_gbi_rand_test(d, N):		
	theta_train = fp.prior_rvs(N)
	qoi_train = [fp.H(theta) for theta in theta_train]
	y_train = [fp.eta(theta, d) for theta in theta_train]
	
	gmm = gbi_train_model(theta_train, qoi_train, y_train, verbose=0, ncomp=8)
	
	truth_theta = fp.prior_rvs(1)
	measured_y = fp.eta(truth_theta, d)
	a,b,c = gbi_condition_model(gmm, measured_y, verbose=0)
	
	plot_predictive_posterior(a, b, c, 0, 7, drawplot=False, plotmean=True)
	plt.axvline(fp.H(truth_theta), c='blue')
	plt.show()

def fp_vv_obed_gbi(d):
	U, U_list = U_varH_gbi(d, fp, n_mc=10**5, n_gmm=10**5, doPrint=True)
	print(U)
	#print(U_list)
	uncertainty_prop_plot(U_list, c='royalblue', xlab="specific U")#, saveFig='OBEDresult')
	return U
	
def fp_jm_test_ols(problem, N):
	###get a validation set
	theta_train = problem.prior_rvs(N)
	d_train = [d for d in problem.sample_d(N)]
	qoi_train = [problem.H(theta) for theta in theta_train]
	y_train = [problem.eta(theta, d) for theta,d in zip(theta_train,d_train)]
	val_input = [yi+di for yi,di in zip(y_train, d_train)]
	val_output = np.array(qoi_train)
	
	###plot the covariance?
	input_cov = np.cov(val_input,rowvar=0)
	plotmatrix(input_cov)
	#print(np.cov(val_output))
	
	###get the model
	model, _ = joint_model_linear(problem, N, doPrint=True)
	
	###calculate what the model calculates for the validation set
	model_output = [model(np.array([vi]))[0] for vi in val_input]
	
	###compare
	validation_error = abs(model_output - val_output)
	#print(val_output)
	#print(model_output)
	uncertainty_prop_plot(validation_error, xlab="model prediction error", c='r')
	


if __name__ == '__main__':  
	#fp_vv_nominal()
	
	#fp_vv_UP_QoI(4.38)
	
	#fp_vv_SA_QoI()
	
	#fp_vv_UP_exp(d_historical)
	#fp_vv_UP_exp(d_best, False)
	#fp_vv_UP_exp(d_worst, False)
	
	#fp_vv_gbi_test(d_historical, y_nominal, 10**6)
	
	#fp_vv_gbi_rand_test(d_historical, 10**4)
	
	#U_hist = fp_vv_obed_gbi(d_historical)
	
	fp_jm_test_ols(fp, 10**4) #doesn't actually seem to get better from 10^2 to 10^4! Probably bc linear model isnt great here (:
	
	"""
	#U_hist = fp_vv_obed_gbi(d_historical)
	U_hist = 0.4981609760099987
	C_hist = np.log(fp.G(d_historical))
	
	#U_best = fp_vv_obed_gbi(d_best)
	U_best = 0.48310825880372715
	C_best = np.log(fp.G(d_best))
	
	#U_worst = fp_vv_obed_gbi(d_worst)
	U_worst = 1.2584888138976271
	C_worst = np.log(fp.G(d_worst))

	pts = [[C_hist, U_hist, "d_hist"], [C_best, U_best, "d_max"], [C_worst, U_worst, "d_min"]]
	plot_ngsa2(costs, utilities, design_pts=pts, showPlot=True, savePlot=False, logPlotXY=[False,False])
	"""
