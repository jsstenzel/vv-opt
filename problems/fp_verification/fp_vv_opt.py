import sys
import os
import scipy.stats
import math
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import csv

sys.path.append('../..')
#focal plane
from problems.fp_verification.fp_problem import *
#analysis
from obed.obed_multivar import *
from obed.mcmc import *
from obed.pdf_estimation import *
from uq.uncertainty_propagation import *
from uq.sensitivity_analysis import *


if __name__ == '__main__':  
	###nominal case
	req = 3.0 #max noise
	print("QoI requirement:", req)

	theta_nominal = [1.1, 2.5, .001]
	QoI_nominal = fp.H(theta_nominal)
	print("Nominal QoI:", QoI_nominal)


	###uncertainty analysis
	if False:
		#uncertainty propagation of HLVA
		uq_thetas = fp.prior_rvs(10000)
		Qs = [fp.H(theta) for theta in uq_thetas]
		uncertainty_prop_plot([theta[0] for theta in uq_thetas], xlab="Gain [ADU/e-]")
		uncertainty_prop_plot([theta[1] for theta in uq_thetas], xlab="Read noise [e-]")
		uncertainty_prop_plot([theta[2] for theta in uq_thetas], xlab="Dark current [e-/s]")
		uncertainty_prop_plot(Qs, xlab="QoI: Avg. Noise [e-]")

		#prob of meeting req along priors:
		count_meetreq = 0
		for Q in Qs:
			if Q <= req:
				count_meetreq += 1
		prob_meetreq = count_meetreq / len(Qs)
		print("Probability of meeting requirement given priors:", prob_meetreq)

	#sensitivity analysis of HLVA
	if False:
		#it'll be straightforward to see the dependence of QoI on theta
		Si = sobol_saltelli(fp.H, 
							2**5, #SALib wants powers of 2 for convergence
							var_names=fp.theta_names, 
							var_dists=[prior[0] for prior in fp.priors], 
							var_bounds=[prior[1] for prior in fp.priors], 
							conf = 0.95, doSijCalc=False, doPlot=True, doPrint=True)


		#less strightforward to do it for the dependence of QoI on d
	
	d_historical = [
					20,   #t_gain
					30,   #I_gain
					1,	#n_meas_rn
					8,	#d_num
					9600, #d_max
					2	 #d_pow   #approx
				   ]
	
	#Uncertainty analysis of the experiment models
	if True:
		print("Likelihood distribution for nominal historical case:",flush=True)
		tt = fp.prior_rvs(1); print(tt)
		ysample_nominal = [fp.eta(tt, d_historical) for _ in range(10000)]
		uncertainty_prop_plots(ysample_nominal, xlabs=["Y0","Y1","Y2"], saveFig='UP_exp_nominal')
		likelihood,_ = general_likelihood_kernel(ysample_nominal)
		#kde_plot(likelihood, ysample_nominal, plotStyle='together') #needs fixing?
		
		#Also plot the y's generated by the joint distribution p(y|theta,d)p(theta)
		print("Experiments simulated from the joint distribution:",flush=True)
		uq_thetas = fp.prior_rvs(10000)
		uq_ys = [fp.eta(theta, d_historical) for theta in uq_thetas]
		uncertainty_prop_plot([y[0] for y in uq_ys], xlab="Y0 (joint distribution)", c='orchid', saveFig='UP_joint_y0')
		uncertainty_prop_plot([y[1] for y in uq_ys], xlab="Y1 (joint distribution)", c='orchid', saveFig='UP_joint_y1.png')
		uncertainty_prop_plot([y[2] for y in uq_ys], xlab="Y2 (joint distribution)", c='orchid', saveFig='UP_joint_y2')
		
	#sensitivity analysis of the experiment models
	if False:
		#we want to see sensitivity of each yi to their inputs, theta and d
		#this is a different framework from before, now theta and d are freewheeling variables
		#(could maybe include some of x in the future...)
		#instead of trying to break eta into its constitutent experiments, i think i want to just use all of eta,
		#and doing separate sensitivity analysis on each of the y[i] coming from theta, over entire d and h span
		#sobol_saltelli expects a function that takes a single list of parameters
		def eta_1(paramlist): #gain
			theta = paramlist[0:fp.dim_theta]
			d = paramlist[fp.dim_theta:fp.dim_theta+fp.dim_d]
			y = fp.eta(theta, d)
			return y[0]
		def eta_2(paramlist): #rn
			theta = paramlist[0:fp.dim_theta]
			d = paramlist[fp.dim_theta:fp.dim_theta+fp.dim_d]
			y = fp.eta(theta, d)
			return y[1]
		def eta_3(paramlist): #dc
			theta = paramlist[0:fp.dim_theta]
			d = paramlist[fp.dim_theta:fp.dim_theta+fp.dim_d]
			y = fp.eta(theta, d)
			return y[2]

		expvar_names = fp.theta_names + fp.d_names
		expvar_dists = [prior[0] for prior in fp.priors] + [prior[0] for prior in fp.d_dists]
		expvar_bounds=[prior[1] for prior in fp.priors] + [prior[1] for prior in fp.d_dists]
			
		Si_1 = sobol_saltelli(eta_1, 
							2**6, #SALib wants powers of 2 for convergence
							var_names=expvar_names, 
							var_dists=expvar_dists, 
							var_bounds=expvar_bounds, 
							conf = 0.99, doSijCalc=False, doPlot=True, doPrint=True)
							
		Si_1 = sobol_saltelli(eta_2, 
							2**6, #SALib wants powers of 2 for convergence
							var_names=expvar_names, 
							var_dists=expvar_dists, 
							var_bounds=expvar_bounds, 
							conf = 0.99, doSijCalc=False, doPlot=True, doPrint=True)
							
		Si_1 = sobol_saltelli(eta_3, 
							2**6, #SALib wants powers of 2 for convergence
							var_names=expvar_names, 
							var_dists=expvar_dists, 
							var_bounds=expvar_bounds, 
							conf = 0.99, doSijCalc=False, doPlot=True, doPrint=True)
							

	###mcmc analysis
	y_nominal = fp_likelihood_fn(dict(zip(fp.theta_names, theta_nominal)), dict(zip(fp.d_names, d_historical)), dict(zip(fp.x_names, fp.x_default)), err=False)
	print(y_nominal)
	
	def proposal_fn_gamma(theta_curr, proposal_width):
		theta_prop = [0] * len(theta_curr)
		for i,_ in enumerate(theta_prop):
			#proposal dists are gammas, to match
			mean = abs(theta_curr[i])
			stddev = proposal_width
			variance = [w**2 for w in stddev]
			alpha = mean**2 / variance
			beta = mean / variance
			theta_prop[i] = scipy.stats.gamma.rvs(size=1, a=alpha, scale=1.0/beta)[0]
		return theta_prop
		
	def proposal_fn_norm(theta_curr, proposal_width):
		theta_prop = [0] * len(theta_curr)
		for i,_ in enumerate(theta_prop):
			#proposal dists are gammas, to match
			mean = abs(theta_curr[i])
			stddev = proposal_width[i]
			theta_prop[i] = scipy.stats.norm.rvs(size=1, loc=mean, scale=stddev)[0]
		return theta_prop
		
	if False: #convergence study with mcmc_multivar
		print("Generating kernel",flush=True)
		n_kde_axis = 47 #47^3 equals about 10^5
		kde_gains = np.linspace(0,3,n_kde_axis)
		kde_rn = np.linspace(1,4,n_kde_axis)
		kde_dc = np.linspace(0,.01,n_kde_axis)
		kde_thetas = np.vstack((np.meshgrid(kde_gains, kde_rn, kde_dc))).reshape(3,-1).T #thanks https://stackoverflow.com/questions/18253210/creating-a-numpy-array-of-3d-coordinates-from-three-1d-arrays
		#uncertainty_prop_plots(kde_thetas, xlabs=['gain','rn','dc'])
		kde_ys = [fp.eta(theta, d_historical) for theta in kde_thetas]
		#uncertainty_prop_plots(kde_ys, xlabs=['Y0','Y1','Y2'])
		likelihood_kernel, kde_ythetas = general_likelihood_kernel(kde_thetas, kde_ys)
		kde_plot(likelihood_kernel, kde_ythetas, plotStyle='together', ynames=['gain','rn','dc','y1','y2','y3']) #this guy is 6d? or 3d? dont know how to plot it
		
		print("mcmc convergence study",flush=True)
		#crucially, this is being evaluated at y=y_nominal. Good and representative, but we'll have to check robustness after.
		n_mcmc = 10**5
		prop_fn = proposal_fn_norm
		#prop_width = [.1,.05,.0001] #rough guess from looking at prior + a few short runs - did 10**5 run
		#prop_width = [5.17612361e-05, 2.53953018e-02, 3.24906620e-07] #from covariance of the above study #this led to 89% acceptance rate, too high! #whoops, used variance instead of stddev
		prop_width = [0.0071945282048228735, 0.15935903430820628, 0.0005700058073427601] #stddev from first study
		mcmc_trace, arate, rrate = mcmc_kernel(y_nominal, likelihood_kernel, prop_fn, prop_width, fp.prior_rvs, fp.prior_pdf_unnorm, n_mcmc, burnin=0, lag=1, doPlot=True, legend=fp.theta_names)
		print(arate, rrate)
		
		#save data, do analysis and plots
		with open('mcmc.csv', 'w', newline='') as csvfile:
			csvwriter = csv.writer(csvfile, delimiter=' ')
			for theta in mcmc_trace:
				csvwriter.writerow(theta)
		print("mean, stddev, covariance of posterior sample:")
		means, stddevs, cov = mcmc_analyze(mcmc_trace,doPlot=True)
		print(means)
		print(stddevs)
		print(cov)
		uncertainty_prop_plot([sample[0] for sample in mcmc_trace], c='limegreen', xlab="Gain [ADU/e-]")
		uncertainty_prop_plot([sample[1] for sample in mcmc_trace], c='limegreen', xlab="Read noise [e-]")
		uncertainty_prop_plot([sample[2] for sample in mcmc_trace], c='limegreen', xlab="Dark current [e-/s]")
	
	#if False: #play with mcmc_nolikelihood
	#	#This is really too slow, so I want to stay away from it
	#	print("mcmc",flush=True)
	#	n_kde = 10**4
	#	n_mcmc = 10**2
	#	prop_fn = proposal_fn_gamma
	#	prop_width = [.3,.3,.002]
	#	mcmc_trace, arate, rrate = mcmc_nolikelihood(y_nominal, d_historical, fp.eta, proposal_fn_norm, fp.prior_rvs, fp.prior_pdf_unnorm, n_mcmc, n_kde, burnin=0, lag=1, doPlot=True, legend=fp.theta_names)
	#	print("acceptance rate",arate, "randomwalk rate", rrate)
		
	###mcmc robustness study
	if False:
		#this value should be a good one derived from the above
		prop_width = [0.007167594573520732, 0.17849464019335232, 0.0006344271319903282]

		print("Generating kernel",flush=True)
		n_kde_axis = 47 #47^3 equals about 10^5
		kde_gains = np.linspace(0,3,n_kde_axis)
		kde_rn = np.linspace(1,4,n_kde_axis)
		kde_dc = np.linspace(0,.01,n_kde_axis)
		kde_thetas = np.vstack((np.meshgrid(kde_gains, kde_rn, kde_dc))).reshape(3,-1).T #thanks https://stackoverflow.com/questions/18253210/creating-a-numpy-array-of-3d-coordinates-from-three-1d-arrays
		kde_ys = [fp.eta(theta, d_historical) for theta in kde_thetas]
		likelihood_kernel, kde_ythetas = general_likelihood_kernel(kde_thetas, kde_ys)
		
		print("mcmc convergence robustness study",flush=True)
		for ysample in [fp.eta(tt, d_historical) for tt in fp.prior_rvs(5)]:
			print("ysample mcmc test:",ysample)
			n_mcmc = 6000
			prop_fn = proposal_fn_norm
			mcmc_trace, arate, rrate = mcmc_kernel(ysample, likelihood_kernel, proposal_fn_norm, prop_width, fp.prior_rvs, fp.prior_pdf_unnorm, n_mcmc, burnin=300, lag=1, doPlot=True, legend=fp.theta_names)
			print(arate, rrate, "            ")
			means, stddevs, cov = mcmc_analyze(mcmc_trace,doPlot=True)
			print("mean of posterior sample", means)
			print("stddev of posterior sample", stddevs)
			print("covariance of posterior sample")
			print(cov)
			uncertainty_prop_plot([sample[0] for sample in mcmc_trace], c='limegreen', xlab="Gain [ADU/e-]")
			uncertainty_prop_plot([sample[1] for sample in mcmc_trace], c='limegreen', xlab="Read noise [e-]")
			uncertainty_prop_plot([sample[2] for sample in mcmc_trace], c='limegreen', xlab="Dark current [e-/s]")
	
	#obed analysis
	#U, U_list = U_probreq(d_historical, fp, maxreq=3.0, n_mc=100, n_mcmc=1000, burnin=0, lag=1)
	#print(U)
	#print(U_list)